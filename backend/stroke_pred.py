
# -*- coding: utf-8 -*-
"""STROKE PRED

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NA6m7y9rIzKfHvKfudTzPqcVCpbfRp2t
"""

import pandas as pd
import os
csv_path = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', 'healthcare-dataset-stroke-data.csv'))
df = pd.read_csv(csv_path)
df = df.drop('id', axis=1)
df.head()

categorical_features = ['gender', 'hypertension',	'heart_disease',	'ever_married',	'work_type','Residence_type',	'smoking_status',	'stroke']
unique_values= {col:df[col].unique() for col in categorical_features}

unique_values

for col, values in unique_values.items():
  print(f"{col}: {values}")

df.duplicated().sum()

df.info()

y=df['stroke']
y.value_counts(normalize=True)

numerical_features = ['age', 'avg_glucose_level', 'bmi']
df[numerical_features].describe()

import plotly.express as px

fig= px.histogram(df,x='gender',width=400)
fig.show()

femst=round(df[df['gender']=='Female']['stroke'].mean()*100,2)
malst=round(df[df['gender']=='Male']['stroke'].mean()*100,2)
print(femst,malst)

fig= px.histogram(df,x='hypertension',width=400)
fig.show()

round(df[df['hypertension']==0]['stroke'].mean()*100,2)

fig= px.histogram(df,x='heart_disease',width=400)
fig.show()

round(df[df['heart_disease']==1]['stroke'].mean()*100,2)

fig= px.histogram(df,x='ever_married',width=400)
fig.show()

round(df[df['ever_married']=='No']['stroke'].mean()*100,2)

fig= px.histogram(df,x='work_type',width=400)
fig.show()

stroke_percentages_work_type= df.groupby('work_type')['stroke'].mean() *100
stroke_percentages_work_type.sort_values(ascending=False)

fig= px.histogram(df,x='Residence_type',width=400)
fig.show()

stroke_percentages_work_type= df.groupby('Residence_type')['stroke'].mean() *100
stroke_percentages_work_type.sort_values(ascending=False)

fig= px.histogram(df,x='smoking_status',width=400)
fig.show()

stroke_percentages_work_type= df.groupby('smoking_status')['stroke'].mean() *100
stroke_percentages_work_type.sort_values(ascending=False)

from sklearn.metrics import mutual_info_score
categorical_features = ['gender', 'hypertension',	'heart_disease',	'ever_married',	'work_type','Residence_type',	'smoking_status']
for col in categorical_features:
  mi= mutual_info_score(df[col],df['stroke'])
  print(f"{col}: {mi:.2f}")

df[numerical_features+['stroke']].corr()

"""Data analysis part is done. The observations are.........Weak positive relationship (0.25) between age and stroke.

>Slight difference in avg. glucose levels between stroke/no stroke.

>BMI shows no significant relation to stroke.

>Hypertension increases stroke risk by 3.3x.

>Males slightly more likely to have stroke than females.

>Heart disease increases stroke risk by 4.07x.

>Married individuals 5.7x more likely to have stroke.

>Self-employed individuals have higher stroke probability.

>Rural residents slightly more likely to have stroke than urban.

>Little difference in stroke risk between smokers and non-smokers.
"""

from sklearn.model_selection import cross_val_score, RepeatedStratifiedKFold
from sklearn.preprocessing import PowerTransformer, OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from imblearn.over_sampling import SMOTE

# Define numerical and categorical features
numerical = ['avg_glucose_level', 'bmi', 'age']
categorical = ['hypertension', 'heart_disease', 'ever_married', 'work_type',
               'Residence_type', 'smoking_status']

# Preprocessing pipeline
transformer = ColumnTransformer(transformers=[
    ('num', Pipeline(steps=[
        ('imputer', SimpleImputer(strategy='median')),
        ('power', PowerTransformer(method='yeo-johnson', standardize=True))
    ]), numerical),

    ('cat', OneHotEncoder(), categorical)
])

from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.model_selection import RepeatedStratifiedKFold, cross_val_score

# Define models
def get_models():
    models, names = list(), list()

    models.append(LogisticRegression(solver='liblinear'))
    names.append('LR')

    models.append(LinearDiscriminantAnalysis())
    names.append('LDA')

    models.append(RandomForestClassifier(n_estimators=100))
    names.append('RF')

    return models, names

# Evaluation function
def evaluate_model(X, y, model):
    cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=42)
    scores = cross_val_score(model, X, y, scoring='roc_auc', cv=cv, n_jobs=-1)
    return scores

import numpy as np
from imblearn.pipeline import Pipeline as IMBPipeline
from imblearn.over_sampling import SMOTE

# Target and features
y = df['stroke']
X = df.drop('stroke', axis=1)

# Get models and their names
models, names = get_models()
results = list()

# Loop through models
for i in range(len(models)):
    IMBpipeline = IMBPipeline(steps=[
        ('transformer', transformer),
        ('smote', SMOTE()),
        ('model', models[i])
    ])

    scores = evaluate_model(X, y, IMBpipeline)
    results.append(scores)

    print('%s: ROC-AUC = %.3f (%.3f)' % (names[i], np.mean(scores), np.std(scores)))